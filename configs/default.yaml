model:
  vision_encoder:
    type: "vit_small"
    patch_size: 16
    hidden_dim: 512
    num_layers: 12
    num_heads: 8
  text_encoder:
    type: "bert_small"
    hidden_dim: 512
    num_layers: 12
    num_heads: 8
  fusion:
    type: "early"
    hidden_dim: 512
    num_layers: 6
    num_heads: 8
  double_loop:
    controller_type: "lstm"
    hidden_dim: 256
    update_frequency: 100
    meta_lr: 1e-5
  heads:
    num_classes: 1000  # For ImageNet-like
    hidden_dim: 512

training:
  micro_batch_size: 4
  gradient_accumulation: 8
  max_epochs: 50
  inner_lr: 3e-4
  warmup_steps: 1000
  mixed_precision: "bf16"
  gradient_checkpointing: true
  optimizer: "adamw"
  scheduler: "cosine"
  weight_decay: 0.01
  max_grad_norm: 1.0

data:
  # Legacy single dataset keys (still supported if `datasets` not defined)
  train_dataset: "coco_captions"
  val_dataset: "coco_captions"
  batch_size: 32
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  shuffle_train: true
  # New multi-dataset selector entries. Each dataset can contribute to splits.
  # Remove or disable with `enabled: false` if not needed.
  datasets:
    - name: multimodal_core
      type: multimodal
      data_dir: ./data/multimodal
      splits:
        train: 0.8
        val: 0.1
        test: 0.1
      enabled: true
    - name: captions_aux
      type: coco_captions
      root: ./data/coco/images
      ann_file: ./data/coco/annotations/captions_train2017.json
      splits:
        train: 1.0
      use_in: [train]
      enabled: true

wolfram:
  api_key: "${WOLFRAM_API_KEY}"
  cache_dir: "./cache/wolfram"
  max_queries_per_day: 2000
  validation_weight: 0.15
  timeout: 30

# Future API integrations (uncomment and configure as needed)
# openai:
#   api_key: "${OPENAI_API_KEY}"
#   model: "gpt-4"
#   max_tokens: 1000
#   temperature: 0.7
#   timeout: 30

# google:
#   api_key: "${GOOGLE_AI_API_KEY}"
#   model: "gemini-pro"
#   max_tokens: 1000
#   temperature: 0.7
#   timeout: 30

# anthropic:
#   api_key: "${ANTHROPIC_API_KEY}"
#   model: "claude-3-sonnet-20240229"
#   max_tokens: 1000
#   temperature: 0.7
#   timeout: 30

# huggingface:
#   api_key: "${HUGGINGFACE_API_TOKEN}"
#   model: "microsoft/DialoGPT-medium"
#   timeout: 30

logging:
  project: "multi-modal-net"
  experiment: "default"
  log_every: 50
  save_every: 5000
  eval_every: 10000

hardware:
  device: "auto"  # Options: "auto", "cuda", "cpu", "npu", "mps", "cuda:0", etc.
  # Set device to "auto" to automatically detect and use best accelerator
  # Set to "cpu" to force CPU training
  # Set to "cuda" for NVIDIA GPU
  # Set to "npu" to use Neural Processing Unit (Intel AI Boost, AMD Ryzen AI, etc.)
  # Set to "mps" for Apple Silicon Neural Engine
  # Set to specific GPU like "cuda:0" for multi-GPU systems
  gpu_id: null  # Specific GPU ID (0, 1, 2, etc.) or null for auto-select
  prefer_npu: false  # If true, prefer NPU over GPU when both available
  max_memory: "11GB"
  compile_model: true
  ddp: false  # Distributed Data Parallel (multi-GPU training)

paths:
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"