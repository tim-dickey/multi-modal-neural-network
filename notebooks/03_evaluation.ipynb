{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93b86ed",
   "metadata": {},
   "source": [
    "# Evaluating the Multi-Modal Neural Network\n",
    "\n",
    "This notebook shows how to load a trained model and run benchmarks on evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e5481",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from src.evaluation.metrics import compute_metrics\n",
    "from src.models import load_model\n",
    "from src.data.dataset import load_eval_dataset\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58951e2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d65154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_path = '../checkpoints/best_model.pth'\n",
    "model = load_model(model_path)\n",
    "model.eval()\n",
    "print(\"Model loaded and set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250538e4",
   "metadata": {},
   "source": [
    "## Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "eval_dataset = load_eval_dataset('../configs/default.yaml')\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=32, shuffle=False)\n",
    "print(f\"Evaluation dataset loaded with {len(eval_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f7cee",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        outputs = model(batch['input'])\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(all_preds, all_labels)\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
