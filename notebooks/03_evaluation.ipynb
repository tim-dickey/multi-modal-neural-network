{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93b86ed",
   "metadata": {},
   "source": [
    "# Evaluating the Multi-Modal Neural Network\n",
    "\n",
    "This notebook shows how to load a trained model and run benchmarks on evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e5481",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from src.training.trainer import Trainer\n",
    "from src.utils.config import load_config\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58951e2",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d65154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and trained model\n",
    "config_path = '../configs/default.yaml'\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Specify checkpoint path (update this to your actual checkpoint)\n",
    "checkpoint_path = '../outputs/checkpoints/best.pt'\n",
    "\n",
    "# Check if checkpoint exists\n",
    "if Path(checkpoint_path).exists():\n",
    "    trainer = Trainer(config_path=config_path, resume_from=checkpoint_path)\n",
    "    model = trainer.model\n",
    "    model.eval()\n",
    "    print(\"Model loaded from checkpoint and set to evaluation mode\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at: {checkpoint_path}\")\n",
    "    print(\"Please train a model first or update the checkpoint path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250538e4",
   "metadata": {},
   "source": [
    "## Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset from config\n",
    "from src.data.dataset import create_dataset_from_config, create_dataloader\n",
    "\n",
    "# Create validation dataset\n",
    "_, val_dataset = create_dataset_from_config(config)\n",
    "val_loader = create_dataloader(\n",
    "    val_dataset, \n",
    "    batch_size=config.get('data', {}).get('batch_size', 32),\n",
    "    shuffle=False,\n",
    "    num_workers=0  # Set to 0 for notebooks\n",
    ")\n",
    "print(f\"Evaluation dataset loaded with {len(val_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f7cee",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63dd10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running evaluation on {device}...\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(val_loader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            images=batch.get('image') or batch.get('images'),\n",
    "            input_ids=batch.get('input_ids'),\n",
    "            attention_mask=batch.get('attention_mask')\n",
    "        )\n",
    "        \n",
    "        logits = outputs['logits']\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        labels = batch.get('label') or batch.get('labels')\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Processed {batch_idx + 1}/{len(val_loader)} batches\")\n",
    "\n",
    "# Compute metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    all_labels, all_preds, average='weighted', zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f06438",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Visualize the confusion matrix and sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "num_classes = len(np.unique(all_labels))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(num_classes), \n",
    "            yticklabels=range(num_classes))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "sample_indices = np.random.choice(len(all_labels), min(10, len(all_labels)), replace=False)\n",
    "for idx in sample_indices:\n",
    "    true_label = all_labels[idx]\n",
    "    pred_label = all_preds[idx]\n",
    "    status = \"✓\" if true_label == pred_label else \"✗\"\n",
    "    print(f\"{status} Sample {idx}: True={true_label}, Predicted={pred_label}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
